//! MIR Statement Code Generation
//!
//! This module compiles MIR statements to LLVM IR.
//!
//! ## Statement Kinds
//!
//! | Statement | Description | Generation Checks |
//! |-----------|-------------|-------------------|
//! | Assign | `place = rvalue` | Via rvalue/place |
//! | StorageLive | Mark local alive | None |
//! | StorageDead | Mark local dead, unregister | Unregister generation |
//! | Drop | Drop value, free memory | None |
//! | IncrementGeneration | Bump generation counter | None |
//! | CaptureSnapshot | Mark for effect capture | None (handled at Perform) |
//! | ValidateGeneration | Check pointer validity | YES |
//! | PushHandler | Install effect handler | None |
//! | PopHandler | Remove effect handler | None |
//! | Nop | No operation | None |

use std::collections::HashMap;

use super::super::llvm::{
    LLVMValueRef, LLVMTypeRef, LLVMBasicBlockRef,
    types::{i8_type, i64_type, ptr_type, void_type, function_type, array_type},
    values::{const_int, const_null, append_basic_block},
};
use super::super::context::{CodegenContext, MemoryTier};
use super::super::lower_type::lower_type;
use super::super::runtime;
use super::rvalue::compile_mir_rvalue;
use super::place::compile_mir_place;
use super::memory;

use crate::hir::{DefId, LocalId, Type};
use crate::mir::{
    MirBody, Statement, StatementKind, Place, Operand,
    EscapeResults, InlineHandlerOp, InlineHandlerCapture,
};
use crate::mir::ptr::MemoryTier as MirMemoryTier;
use crate::mir::static_evidence::InlineEvidenceMode;
use crate::effects::evidence::HandlerStateKind;
use crate::diagnostics::Diagnostic;
use crate::span::Span;

// ============================================================================
// Statement Compilation
// ============================================================================

/// Compile a single MIR statement to LLVM IR.
pub fn compile_mir_statement(
    ctx: &mut CodegenContext,
    stmt: &Statement,
    body: &MirBody,
    escape_results: Option<&EscapeResults>,
) -> Result<(), Vec<Diagnostic>> {
    match &stmt.kind {
        StatementKind::Assign(place, rvalue) => {
            compile_assign(ctx, place, rvalue, body, escape_results, stmt.span)
        }

        StatementKind::StorageLive(local) => {
            // Storage annotations - can be used for LLVM lifetime intrinsics
            // For now, no-op since we allocate at function start
            let _ = local;
            Ok(())
        }

        StatementKind::StorageDead(local) => {
            compile_storage_dead(ctx, *local, body, stmt.span)
        }

        StatementKind::Drop(place) => {
            compile_drop(ctx, place, body, escape_results, stmt.span)
        }

        StatementKind::IncrementGeneration(place) => {
            compile_increment_generation(ctx, place, body, escape_results, stmt.span)
        }

        StatementKind::CaptureSnapshot(local) => {
            // CaptureSnapshot is intentionally a no-op in codegen.
            // Snapshot lifecycle is handled entirely by the Perform terminator:
            // 1. Perform creates snapshot via blood_snapshot_create()
            // 2. Perform iterates effect-captured locals from escape analysis
            // 3. For each captured local, Perform calls blood_snapshot_add_entry()
            // 4. After effect operation returns, Perform validates snapshot
            // 5. Perform destroys snapshot via blood_snapshot_destroy()
            let _ = local;
            Ok(())
        }

        StatementKind::ValidateGeneration { ptr, expected_gen } => {
            compile_validate_generation(ctx, ptr, expected_gen, body, escape_results, stmt.span)
        }

        StatementKind::PushHandler { handler_id, state_place, state_kind, allocation_tier, inline_mode } => {
            compile_push_handler(
                ctx, *handler_id, state_place, state_kind, allocation_tier, inline_mode,
                body, stmt.span
            )
        }

        StatementKind::PushInlineHandler { effect_id, operations, allocation_tier, inline_mode } => {
            compile_push_inline_handler(
                ctx, *effect_id, operations, allocation_tier, inline_mode,
                body, escape_results, stmt.span
            )
        }

        StatementKind::PopHandler => {
            compile_pop_handler(ctx, stmt.span)
        }

        StatementKind::CallReturnClause { handler_id, handler_name, body_result, state_place, destination } => {
            compile_call_return_clause(
                ctx, *handler_id, handler_name, body_result, state_place, destination,
                body, escape_results, stmt.span
            )
        }

        StatementKind::Nop => {
            // No operation
            Ok(())
        }
    }
}

// ============================================================================
// Assignment
// ============================================================================

/// Compile an assignment: `place = rvalue`
fn compile_assign(
    ctx: &mut CodegenContext,
    place: &Place,
    rvalue: &crate::mir::Rvalue,
    body: &MirBody,
    escape_results: Option<&EscapeResults>,
    span: Span,
) -> Result<(), Vec<Diagnostic>> {
    // Compile rvalue to get the value
    let value = compile_mir_rvalue(ctx, rvalue, body, escape_results)?;

    // Compile place to get the destination pointer
    let ptr = compile_mir_place(ctx, place, body, escape_results)?;

    // Store the value
    ctx.llvm_builder().build_store(value, ptr);

    Ok(())
}

// ============================================================================
// Storage Dead
// ============================================================================

/// Compile StorageDead: unregister generation if region-allocated.
fn compile_storage_dead(
    ctx: &mut CodegenContext,
    local: LocalId,
    body: &MirBody,
    span: Span,
) -> Result<(), Vec<Diagnostic>> {
    // If this local was region-allocated (has entry in local_generations),
    // we must unregister it to invalidate its generation. This enables
    // use-after-free detection: any subsequent dereference with the old
    // generation will fail validation.
    if let Some(&gen_alloca) = ctx.local_generations.get(&local) {
        if let Some(&local_ptr) = ctx.locals.get(&local) {
            let i64_ty = i64_type(ctx.llvm_context_ref());

            // Load the address from the local's storage
            let loaded = ctx.llvm_builder().build_load(
                ptr_type(ctx.llvm_context_ref()),
                local_ptr,
                "local_addr\0"
            );

            // Convert to i64 address for unregister call
            let address = ctx.llvm_builder().build_ptr_to_int(loaded, i64_ty, "addr_for_unregister\0");

            // Call blood_unregister_allocation to invalidate the generation
            let unregister_fn = ctx.get_runtime_fn("blood_unregister_allocation")
                .ok_or_else(|| vec![Diagnostic::error(
                    "Runtime function blood_unregister_allocation not found",
                    span
                )])?;

            ctx.llvm_builder().build_call(
                unregister_fn,
                &[address],
                "\0"
            );
        }
    }

    Ok(())
}

// ============================================================================
// Drop
// ============================================================================

/// Compile Drop: free memory if heap allocated.
fn compile_drop(
    ctx: &mut CodegenContext,
    place: &Place,
    body: &MirBody,
    escape_results: Option<&EscapeResults>,
    span: Span,
) -> Result<(), Vec<Diagnostic>> {
    // Get the address of the place
    let ptr = compile_mir_place(ctx, place, body, escape_results)?;

    // Get the type to determine size
    let place_ty = &body.locals[place.local.index as usize].ty;
    let llvm_ty = lower_type(ctx, place_ty);
    let i64_ty = i64_type(ctx.llvm_context_ref());

    // Calculate size (use target data if available, otherwise use 0)
    let size = if let Some(target_data) = ctx.target_data() {
        let size_val = target_data.size_of_type(llvm_ty);
        const_int(i64_ty, size_val, false)
    } else {
        const_int(i64_ty, 0, false)
    };

    // For reference types, call blood_free to deallocate
    if place_ty.is_ref() {
        if let Some(free_fn) = ctx.get_runtime_fn("blood_free") {
            // Load the pointer value
            let ptr_val = ctx.llvm_builder().build_load(
                ptr_type(ctx.llvm_context_ref()),
                ptr,
                "drop_val\0"
            );

            // Convert to i64 address
            let address = ctx.llvm_builder().build_ptr_to_int(ptr_val, i64_ty, "drop_addr\0");

            // Call blood_free(address, size)
            ctx.llvm_builder().build_call(free_fn, &[address, size], "\0");
        }
    }
    // For non-reference types, no deallocation needed

    Ok(())
}

// ============================================================================
// Generation Management
// ============================================================================

/// Compile IncrementGeneration: bump the generation counter.
fn compile_increment_generation(
    ctx: &mut CodegenContext,
    place: &Place,
    body: &MirBody,
    escape_results: Option<&EscapeResults>,
    span: Span,
) -> Result<(), Vec<Diagnostic>> {
    let ptr = compile_mir_place(ctx, place, body, escape_results)?;

    let increment_fn = ctx.get_runtime_fn("blood_increment_generation")
        .ok_or_else(|| vec![Diagnostic::error(
            "Runtime function blood_increment_generation not found. \
             IncrementGeneration requires this function to be declared.",
            span
        )])?;

    ctx.llvm_builder().build_call(increment_fn, &[ptr], "\0");

    Ok(())
}

/// Compile ValidateGeneration: check pointer validity.
fn compile_validate_generation(
    ctx: &mut CodegenContext,
    ptr: &Place,
    expected_gen: &Operand,
    body: &MirBody,
    escape_results: Option<&EscapeResults>,
    span: Span,
) -> Result<(), Vec<Diagnostic>> {
    // Check if generation validation can be skipped based on escape analysis.
    // For stack-allocated values (NoEscape), generation checks are unnecessary
    // because the reference is guaranteed valid within the scope.
    let should_skip = if let Some(results) = escape_results {
        let local = ptr.local;
        results.stack_promotable.contains(&local)
    } else {
        false
    };

    if !should_skip {
        let ptr_val = compile_mir_place(ctx, ptr, body, escape_results)?;
        let expected = compile_mir_operand(ctx, expected_gen, body, escape_results)?;

        memory::emit_generation_check(ctx, ptr_val, expected, span)?;
    }

    Ok(())
}

// ============================================================================
// Effect Handler Management
// ============================================================================

/// Compile PushHandler: install effect handler onto evidence vector.
fn compile_push_handler(
    ctx: &mut CodegenContext,
    handler_id: DefId,
    state_place: &Place,
    state_kind: &HandlerStateKind,
    allocation_tier: &MirMemoryTier,
    inline_mode: &InlineEvidenceMode,
    body: &MirBody,
    span: Span,
) -> Result<(), Vec<Diagnostic>> {
    let i64_ty = i64_type(ctx.llvm_context_ref());
    let i8_ptr_ty = ptr_type(ctx.llvm_context_ref());

    // Look up the handler's effect_id
    let handler_info = ctx.handler_defs.get(&handler_id).ok_or_else(|| {
        vec![Diagnostic::error(
            format!("Internal error: no handler info for DefId({})", handler_id.index),
            span,
        )]
    })?;
    let effect_id = handler_info.effect_id;

    // Get the state pointer from state_place
    let state_ptr = *ctx.locals.get(&state_place.local).ok_or_else(|| {
        vec![Diagnostic::error(
            format!("Local _{} not found for handler state", state_place.local.index),
            span,
        )]
    })?;

    // Get evidence runtime functions
    let ev_current = ctx.get_or_declare_evidence_fn("blood_evidence_current");
    let ev_create = ctx.get_or_declare_evidence_fn("blood_evidence_create");
    let ev_push_with_state = ctx.get_or_declare_evidence_fn("blood_evidence_push_with_state");
    let ev_set_current = ctx.get_or_declare_evidence_fn("blood_evidence_set_current");

    // Get current evidence vector
    let current_ev = ctx.llvm_builder().build_call(ev_current, &[], "current_ev\0");

    // Check if current evidence is null
    let is_null = ctx.llvm_builder().build_is_null(current_ev, "ev_is_null\0");

    let current_fn = ctx.current_fn().ok_or_else(|| {
        vec![Diagnostic::error("No current function", span)]
    })?;

    // STACK ALLOCATION OPTIMIZATION (EFF-OPT-005/006):
    // For stack-tier handlers, push directly onto existing evidence without cloning
    let ev = if *allocation_tier == MirMemoryTier::Stack {
        compile_stack_tier_evidence_creation(ctx, current_fn, current_ev, is_null, ev_create, ev_set_current, span)?
    } else {
        compile_region_tier_evidence_creation(ctx, current_fn, current_ev, is_null, ev_create, ev_set_current, span)?
    };

    // Push handler with effect_id and state pointer
    let effect_id_val = const_int(i64_ty, effect_id.index as u64, false);
    let state_void_ptr = ctx.llvm_builder().build_bitcast(state_ptr, i8_ptr_ty, "state_void_ptr\0");

    ctx.llvm_builder().build_call(
        ev_push_with_state,
        &[ev, effect_id_val, state_void_ptr],
        "\0"
    );

    Ok(())
}

/// Create evidence for stack-tier handlers (no cloning).
fn compile_stack_tier_evidence_creation(
    ctx: &mut CodegenContext,
    current_fn: LLVMValueRef,
    current_ev: LLVMValueRef,
    is_null: LLVMValueRef,
    ev_create: LLVMValueRef,
    ev_set_current: LLVMValueRef,
    span: Span,
) -> Result<LLVMValueRef, Vec<Diagnostic>> {
    let i8_ptr_ty = ptr_type(ctx.llvm_context_ref());

    let create_block = append_basic_block(ctx.llvm_context_ref(), current_fn, "stack_create_ev\0");
    let use_block = append_basic_block(ctx.llvm_context_ref(), current_fn, "stack_use_ev\0");
    let merge_block = append_basic_block(ctx.llvm_context_ref(), current_fn, "stack_merge_ev\0");

    // Branch: if null, create new evidence; else use existing directly
    ctx.llvm_builder().build_cond_br(is_null, create_block.raw(), use_block.raw());

    // Create block: evidence is null, create new one
    ctx.llvm_builder().position_at_end(create_block.raw());
    let new_ev = ctx.llvm_builder().build_call(ev_create, &[], "new_evidence\0");
    ctx.llvm_builder().build_call(ev_set_current, &[new_ev], "\0");
    ctx.llvm_builder().build_br(merge_block.raw());
    let create_block_end = ctx.llvm_builder().get_insert_block();

    // Use block: evidence exists, use it directly (NO clone)
    ctx.llvm_builder().position_at_end(use_block.raw());
    ctx.llvm_builder().build_br(merge_block.raw());
    let use_block_end = ctx.llvm_builder().get_insert_block();

    // Merge block: phi to select the evidence pointer
    ctx.llvm_builder().position_at_end(merge_block.raw());
    let ev_phi = ctx.llvm_builder().build_phi(i8_ptr_ty, "evidence\0");
    ctx.llvm_builder().add_phi_incoming(ev_phi, &[
        (new_ev, create_block_end),
        (current_ev, use_block_end),
    ]);

    Ok(ev_phi)
}

/// Create evidence for region-tier handlers (clone existing).
fn compile_region_tier_evidence_creation(
    ctx: &mut CodegenContext,
    current_fn: LLVMValueRef,
    current_ev: LLVMValueRef,
    is_null: LLVMValueRef,
    ev_create: LLVMValueRef,
    ev_set_current: LLVMValueRef,
    span: Span,
) -> Result<LLVMValueRef, Vec<Diagnostic>> {
    let i8_ptr_ty = ptr_type(ctx.llvm_context_ref());

    let create_block = append_basic_block(ctx.llvm_context_ref(), current_fn, "region_create_ev\0");
    let clone_block = append_basic_block(ctx.llvm_context_ref(), current_fn, "region_clone_ev\0");
    let merge_block = append_basic_block(ctx.llvm_context_ref(), current_fn, "region_merge_ev\0");

    // Branch: if null, create new; else clone existing
    ctx.llvm_builder().build_cond_br(is_null, create_block.raw(), clone_block.raw());

    // Create block
    ctx.llvm_builder().position_at_end(create_block.raw());
    let new_ev = ctx.llvm_builder().build_call(ev_create, &[], "new_evidence\0");
    ctx.llvm_builder().build_br(merge_block.raw());
    let create_block_end = ctx.llvm_builder().get_insert_block();

    // Clone block: blood_evidence_create clones when one exists
    ctx.llvm_builder().position_at_end(clone_block.raw());
    let cloned_ev = ctx.llvm_builder().build_call(ev_create, &[], "cloned_evidence\0");
    ctx.llvm_builder().build_br(merge_block.raw());
    let clone_block_end = ctx.llvm_builder().get_insert_block();

    // Merge block
    ctx.llvm_builder().position_at_end(merge_block.raw());
    let ev_phi = ctx.llvm_builder().build_phi(i8_ptr_ty, "evidence\0");
    ctx.llvm_builder().add_phi_incoming(ev_phi, &[
        (new_ev, create_block_end),
        (cloned_ev, clone_block_end),
    ]);

    // Set as current (needed for region-tier since we cloned)
    ctx.llvm_builder().build_call(ev_set_current, &[ev_phi], "\0");

    Ok(ev_phi)
}

/// Compile PushInlineHandler: install inline effect handler.
fn compile_push_inline_handler(
    ctx: &mut CodegenContext,
    effect_id: DefId,
    operations: &[InlineHandlerOp],
    allocation_tier: &MirMemoryTier,
    inline_mode: &InlineEvidenceMode,
    body: &MirBody,
    escape_results: Option<&EscapeResults>,
    span: Span,
) -> Result<(), Vec<Diagnostic>> {
    let i64_ty = i64_type(ctx.llvm_context_ref());
    let i8_ptr_ty = ptr_type(ctx.llvm_context_ref());

    // Get evidence runtime functions
    let ev_current = ctx.get_or_declare_evidence_fn("blood_evidence_current");
    let ev_create = ctx.get_or_declare_evidence_fn("blood_evidence_create");
    let ev_set_current = ctx.get_or_declare_evidence_fn("blood_evidence_set_current");

    // Get current evidence vector
    let current_ev = ctx.llvm_builder().build_call(ev_current, &[], "current_ev\0");
    let is_null = ctx.llvm_builder().build_is_null(current_ev, "ev_is_null\0");

    let current_fn = ctx.current_fn().ok_or_else(|| {
        vec![Diagnostic::error("No current function", span)]
    })?;

    // Handle evidence creation based on allocation tier
    let ev = if *allocation_tier == MirMemoryTier::Stack {
        compile_stack_tier_evidence_creation(ctx, current_fn, current_ev, is_null, ev_create, ev_set_current, span)?
    } else {
        compile_region_tier_evidence_creation(ctx, current_fn, current_ev, is_null, ev_create, ev_set_current, span)?
    };

    // Compile inline handler operations and register with runtime
    let ev_register = ctx.get_or_declare_evidence_fn("blood_evidence_register");

    // Save current insertion point
    let saved_block = ctx.llvm_builder().get_insert_block();

    // Compile all handler operations and collect function pointers
    let mut compiled_ops: Vec<LLVMValueRef> = Vec::new();

    for op in operations {
        if let Some(handler_body) = ctx.inline_handler_bodies.get(&op.synthetic_fn_def_id).cloned() {
            let handler_fn = compile_inline_handler_op_body(ctx, op.synthetic_fn_def_id, &handler_body)?;
            compiled_ops.push(handler_fn);
        } else {
            // Handler body not found - use null pointer
            compiled_ops.push(const_null(i8_ptr_ty));
        }
    }

    // Restore insertion point
    ctx.llvm_builder().position_at_end(saved_block);

    // Build array of operation function pointers on stack
    let op_count = operations.len();

    if op_count > 0 {
        let array_ty = array_type(i8_ptr_ty, op_count as u32);
        let ops_array = ctx.llvm_builder().build_alloca(array_ty, "ops_array\0");

        // Store each function pointer in the array
        for (i, fn_ptr) in compiled_ops.iter().enumerate() {
            let indices = [
                const_int(i64_ty, 0, false),
                const_int(i64_ty, i as u64, false),
            ];
            let elem_ptr = ctx.llvm_builder().build_gep(array_ty, ops_array, &indices, &format!("op_ptr_{}\0", i));
            let fn_as_ptr = ctx.llvm_builder().build_bitcast(*fn_ptr, i8_ptr_ty, &format!("fn_ptr_{}\0", i));
            ctx.llvm_builder().build_store(fn_as_ptr, elem_ptr);
        }

        // Cast array pointer to **void
        let ptr_ptr_ty = ptr_type(ctx.llvm_context_ref());
        let ops_ptr = ctx.llvm_builder().build_bitcast(ops_array, ptr_ptr_ty, "ops_ptr\0");

        // Register all operations with the evidence system
        let effect_id_val = const_int(i64_ty, effect_id.index as u64, false);
        let op_count_val = const_int(i64_ty, op_count as u64, false);

        ctx.llvm_builder().build_call(
            ev_register,
            &[ev, effect_id_val, ops_ptr, op_count_val],
            "\0"
        );
    }

    // Build captures struct for handler state
    let all_captures = collect_unique_captures(operations);

    let state_ptr = if all_captures.is_empty() {
        const_null(i8_ptr_ty)
    } else {
        compile_captures_struct(ctx, &all_captures, span)?
    };

    // Set state for the handler entry
    if !all_captures.is_empty() {
        let ev_set_state = ctx.get_or_declare_evidence_fn("blood_evidence_set_state");
        ctx.llvm_builder().build_call(ev_set_state, &[ev, state_ptr], "\0");
    }

    Ok(())
}

/// Collect unique captures across all inline handler operations.
fn collect_unique_captures<'a>(operations: &'a [InlineHandlerOp]) -> Vec<&'a InlineHandlerCapture> {
    let mut all_captures: Vec<&InlineHandlerCapture> = Vec::new();
    let mut seen_locals = std::collections::HashSet::new();

    for op in operations {
        for capture in &op.captures {
            if seen_locals.insert(capture.local_id) {
                all_captures.push(capture);
            }
        }
    }

    all_captures
}

/// Compile captures struct for inline handler.
fn compile_captures_struct(
    ctx: &mut CodegenContext,
    captures: &[&InlineHandlerCapture],
    span: Span,
) -> Result<LLVMValueRef, Vec<Diagnostic>> {
    let i64_ty = i64_type(ctx.llvm_context_ref());
    let i8_ptr_ty = ptr_type(ctx.llvm_context_ref());

    let captures_count = captures.len();
    let captures_array_ty = array_type(i8_ptr_ty, captures_count as u32);
    let captures_alloca = ctx.llvm_builder().build_alloca(captures_array_ty, "captures\0");

    for (idx, capture) in captures.iter().enumerate() {
        let local_ptr = ctx.locals.get(&capture.local_id)
            .ok_or_else(|| vec![Diagnostic::error(
                format!("Captured local {:?} not found in current scope", capture.local_id),
                span
            )])?;

        let indices = [
            const_int(i64_ty, 0, false),
            const_int(i64_ty, idx as u64, false),
        ];
        let elem_ptr = ctx.llvm_builder().build_gep(
            captures_array_ty, captures_alloca, &indices,
            &format!("capture_{}_slot\0", idx)
        );

        let local_as_ptr = ctx.llvm_builder().build_bitcast(*local_ptr, i8_ptr_ty, &format!("capture_{}_ptr\0", idx));
        ctx.llvm_builder().build_store(local_as_ptr, elem_ptr);
    }

    Ok(ctx.llvm_builder().build_bitcast(captures_alloca, i8_ptr_ty, "captures_ptr\0"))
}

/// Compile an inline handler operation body to an LLVM function.
fn compile_inline_handler_op_body(
    ctx: &mut CodegenContext,
    def_id: DefId,
    handler_body: &crate::mir::MirBody,
) -> Result<LLVMValueRef, Vec<Diagnostic>> {
    // This will be implemented to compile the handler body as a separate function
    // For now, return error indicating not yet implemented
    Err(vec![Diagnostic::error(
        "Inline handler body compilation not yet implemented",
        handler_body.span
    )])
}

/// Compile PopHandler: remove effect handler from evidence vector.
fn compile_pop_handler(
    ctx: &mut CodegenContext,
    span: Span,
) -> Result<(), Vec<Diagnostic>> {
    let i8_ptr_ty = ptr_type(ctx.llvm_context_ref());

    let ev_pop = ctx.get_or_declare_evidence_fn("blood_evidence_pop");
    let ev_current = ctx.get_or_declare_evidence_fn("blood_evidence_current");

    // Get current evidence vector
    let ev = ctx.llvm_builder().build_call(ev_current, &[], "current_ev\0");

    // Pop the handler
    ctx.llvm_builder().build_call(ev_pop, &[ev], "\0");

    Ok(())
}

/// Compile CallReturnClause: call handler's return clause.
fn compile_call_return_clause(
    ctx: &mut CodegenContext,
    handler_id: DefId,
    handler_name: &str,
    body_result: &Operand,
    state_place: &Place,
    destination: &Place,
    body: &MirBody,
    escape_results: Option<&EscapeResults>,
    span: Span,
) -> Result<(), Vec<Diagnostic>> {
    let i64_ty = i64_type(ctx.llvm_context_ref());
    let i8_ptr_ty = ptr_type(ctx.llvm_context_ref());

    // Generate return clause function name
    let return_fn_name = format!("{}_return\0", handler_name);

    // Declare or get the return clause function
    let return_fn = ctx.get_or_declare_fn(&return_fn_name, || {
        // Signature: fn(result: i64, state_ptr: *void) -> i64
        let param_types = [i64_ty, i8_ptr_ty];
        function_type(i64_ty, &param_types, false)
    });

    // Compile the body result operand
    let body_result_val = compile_mir_operand(ctx, body_result, body, escape_results)?;

    // Convert body result to i64
    let result_i64 = ctx.llvm_builder().build_int_cast(body_result_val, i64_ty, true, "result_ext\0");

    // Get state pointer
    let state_ptr = compile_mir_place(ctx, state_place, body, escape_results)?;
    let state_void_ptr = ctx.llvm_builder().build_bitcast(state_ptr, i8_ptr_ty, "state_void_ptr\0");

    // Call return clause
    let call_result = ctx.llvm_builder().build_call(
        return_fn,
        &[result_i64, state_void_ptr],
        "return_clause_result\0"
    );

    // Store result in destination
    let dest_ptr = compile_mir_place(ctx, destination, body, escape_results)?;

    // Get destination type and convert if needed
    let dest_ty = &body.locals[destination.local.index as usize].ty;
    let dest_llvm_ty = lower_type(ctx, dest_ty);

    let converted_result = convert_i64_to_type(ctx, call_result, dest_llvm_ty, span)?;
    ctx.llvm_builder().build_store(converted_result, dest_ptr);

    Ok(())
}

/// Convert an i64 value to a target LLVM type.
fn convert_i64_to_type(
    ctx: &mut CodegenContext,
    value: LLVMValueRef,
    target_ty: LLVMTypeRef,
    span: Span,
) -> Result<LLVMValueRef, Vec<Diagnostic>> {
    let i64_ty = i64_type(ctx.llvm_context_ref());

    // Check target type kind and convert appropriately
    // This is a simplified version - full implementation would check type kinds
    // For now, return the value as-is (most cases will be i64)
    Ok(value)
}

// ============================================================================
// Operand Compilation
// ============================================================================

/// Compile an MIR operand to an LLVM value.
pub fn compile_mir_operand(
    ctx: &mut CodegenContext,
    operand: &Operand,
    body: &MirBody,
    escape_results: Option<&EscapeResults>,
) -> Result<LLVMValueRef, Vec<Diagnostic>> {
    match operand {
        Operand::Copy(place) | Operand::Move(place) => {
            // Load value from place
            let ptr = compile_mir_place(ctx, place, body, escape_results)?;
            let place_ty = get_place_type(body, place);
            let llvm_ty = lower_type(ctx, &place_ty);
            Ok(ctx.llvm_builder().build_load(llvm_ty, ptr, "operand\0"))
        }

        Operand::Constant(constant) => {
            compile_constant(ctx, constant)
        }
    }
}

/// Get the type of a place.
fn get_place_type(body: &MirBody, place: &Place) -> Type {
    // Start with the local's type
    let mut ty = body.locals[place.local.index as usize].ty.clone();

    // Apply projections
    for elem in &place.projection {
        ty = apply_projection_type(&ty, elem);
    }

    ty
}

/// Apply a projection element to get the resulting type.
fn apply_projection_type(ty: &Type, elem: &crate::mir::PlaceElem) -> Type {
    use crate::mir::PlaceElem;

    match elem {
        PlaceElem::Deref => {
            // Dereference: get the pointee type
            if let Some(inner) = ty.pointee_type() {
                inner.clone()
            } else {
                ty.clone()
            }
        }
        PlaceElem::Field(idx) => {
            // Field access: get the field type
            if let Some(field_ty) = ty.field_type(*idx as usize) {
                field_ty.clone()
            } else {
                ty.clone()
            }
        }
        PlaceElem::Index(_) | PlaceElem::ConstantIndex { .. } => {
            // Array/slice indexing: get element type
            if let Some(elem_ty) = ty.element_type() {
                elem_ty.clone()
            } else {
                ty.clone()
            }
        }
        PlaceElem::Subslice { .. } => {
            // Subslice: same type
            ty.clone()
        }
        PlaceElem::Downcast(_) => {
            // Enum downcast: same type (variant-specific access comes from field)
            ty.clone()
        }
    }
}

/// Compile a constant to an LLVM value.
fn compile_constant(
    ctx: &mut CodegenContext,
    constant: &crate::mir::Constant,
) -> Result<LLVMValueRef, Vec<Diagnostic>> {
    use crate::mir::ConstantKind;
    use super::super::llvm::values::{const_int, const_real, const_string};

    let llvm_ty = lower_type(ctx, &constant.ty);

    match &constant.kind {
        ConstantKind::Int(v) => {
            Ok(const_int(llvm_ty, *v as u64, true))
        }
        ConstantKind::Uint(v) => {
            Ok(const_int(llvm_ty, *v as u64, false))
        }
        ConstantKind::Float(v) => {
            Ok(const_real(llvm_ty, *v))
        }
        ConstantKind::Bool(v) => {
            let i1_ty = super::super::llvm::types::i1_type(ctx.llvm_context_ref());
            Ok(const_int(i1_ty, if *v { 1 } else { 0 }, false))
        }
        ConstantKind::Char(c) => {
            let i32_ty = super::super::llvm::types::i32_type(ctx.llvm_context_ref());
            Ok(const_int(i32_ty, *c as u64, false))
        }
        ConstantKind::String(s) => {
            Ok(const_string(ctx.llvm_context_ref(), s, true))
        }
        ConstantKind::ByteString(bytes) => {
            // Create byte array constant
            let i8_ty = super::super::llvm::types::i8_type(ctx.llvm_context_ref());
            let values: Vec<LLVMValueRef> = bytes.iter()
                .map(|b| const_int(i8_ty, *b as u64, false))
                .collect();
            Ok(super::super::llvm::values::const_array(i8_ty, &values))
        }
        ConstantKind::Unit => {
            // Unit is represented as an empty struct
            Ok(super::super::llvm::values::const_struct(ctx.llvm_context_ref(), &[], false))
        }
        ConstantKind::FnDef(def_id) => {
            // Function reference
            if let Some(&fn_val) = ctx.functions.get(def_id) {
                Ok(fn_val)
            } else {
                Err(vec![Diagnostic::error(
                    format!("Function {:?} not found", def_id),
                    crate::span::Span::dummy()
                )])
            }
        }
        ConstantKind::ConstDef(def_id) | ConstantKind::StaticDef(def_id) => {
            // Const/static reference
            if let Some(&val) = ctx.constants.get(def_id) {
                Ok(val)
            } else {
                Err(vec![Diagnostic::error(
                    format!("Constant/static {:?} not found", def_id),
                    crate::span::Span::dummy()
                )])
            }
        }
        ConstantKind::ZeroSized => {
            // Zero-sized type: return undef
            Ok(super::super::llvm::values::get_undef(llvm_ty))
        }
    }
}
